/*
 * This source file was generated by the Gradle 'init' task
 */
package org.lib;

import java.lang.Math;
import java.util.Arrays;
import java.util.Comparator;
import java.util.stream.Collectors;

class Centroid {
    public int mean;
    public int weight;

    Centroid(int mean, int weight) {
        this.mean = mean;
        this.weight = weight;
    }

    public void add(int value, int weight) {
        final int combinedWeight = this.weight + weight;
        final int combinedMean = (this.mean * this.weight + value * weight) / combinedWeight;

        this.mean = combinedMean;
        this.weight = combinedWeight;
    }
}


/**
 * T-Digest: A T-Digest (short for “tree digest”) is an approximate data structure that trades off
 * accuracy for a low memory footprint and is particularly useful when dealing with large datasets
 * or real-time data analysis where traditional sorting-based approaches become impractical.
 * It provides an approximation of the underlying distribution of the data, allowing for efficient
 * and accurate estimation of percentiles.
 * 
 * Pros: Accurate estimation of percentiles and quantiles, memory-efficient.
 * Cons: Slower update operations, approximation error for extreme quantiles.
 * 
 * Use Cases:
 * 
 * Real-time analytics: T-Digest is efficient for estimating percentiles in real-time data analysis scenarios.
 * For example, in monitoring systems, it can be used to track the distribution of response times, allowing
 * developers to quickly identify anomalies or performance issues.
 * A/B testing: When conducting A/B tests on large-scale systems, T-Digest can be used to estimate percentiles
 * of various metrics, such as conversion rates or click-through rates. It enables efficient and accurate
 * analysis of experimental results, even with large sample sizes.
 * Machine learning: T-Digest can be applied in machine learning pipelines to estimate quantiles of features
 * or predictions. It is useful in scenarios where a large dataset needs to be summarized efficiently, such as
 * outlier detection, data preprocessing, or constructing histograms for model interpretation.
 */
public class TDigest {
    private int compression;
    private Centroid[] centroids;
    private int size = 0;

    TDigest(int compression) {
        this.compression = compression;
        this.centroids = new Centroid[compression];
    }

    public void add(int value, int weight) {
        // Step 1: Find the nearest centroid to the given value
        final int nearestCentroid = this.findNearestCentroid(value);

        if (nearestCentroid != -1) {
            // Step 2: Update the nearest centroid by adding the weight
            this.centroids[nearestCentroid].add(value, weight);
        } else {
            // Step 3: Create a new centroid if no nearest centroid found
            var newCentroid = new Centroid(value, weight);
            // this.centroids.push(newCentroid);
            this.centroids[size++] = newCentroid;
        }

        // Step 4: Merge nearby centroids to maintain compression
        this.mergeCentroids();
    }

    public int findNearestCentroid(int value) {
        int nearestCentroid = -1;
        int minDistance = Integer.MAX_VALUE;

        for (int i = 0; i < this.centroids.length; i++) {
            if(this.centroids[i] != null){
            final int distance = Math.abs(this.centroids[i].mean - value);

            if (distance < minDistance) {
                minDistance = distance;
                nearestCentroid = i;
            }
            }
        }

        return nearestCentroid;
    }

    public void mergeCentroids() {

        // filtrar por não-nulos;
        Centroid[] arrCentroids = Arrays.stream(this.centroids)
            .filter(obj -> obj != null)
            .toArray(Centroid[]::new);
        // ordenar objetos por propriedade;
        Arrays.sort(arrCentroids, Comparator.comparingInt(p -> p.mean));

        int i = 0;
        while (i < this.centroids.length - 1 && (this.centroids[i] != null && this.centroids[i+1] != null)) {
            Centroid curr = this.centroids[i];
            Centroid next = this.centroids[i + 1];

            final int combinedWeight = curr.weight + next.weight;
            final int combinedMean = (curr.mean * curr.weight + next.mean * next.weight) / combinedWeight;

            if ((next.mean - curr.mean) * combinedWeight <= this.compression) {
                curr.mean = combinedMean;
                curr.weight = combinedWeight;
                // this.centroids.splice(i + 1, 1);
                this.centroids[i + 1] = null;
            } else {
                i++;
            }
        }
    }

    public int estimateQuantile(double quantile) {
        final double desiredWeight = quantile * this.getTotalWeight();
        int cumulativeWeight = 0;

        for (Centroid centroid : this.centroids) {
            if(centroid != null){
                cumulativeWeight += centroid.weight;

                if (cumulativeWeight >= desiredWeight) {
                    return centroid.mean;
                }
            }
        }

        return this.centroids[this.centroids.length - 1].mean;
    }

    public int getTotalWeight() {
        int totalWeight = 0;

        for (Centroid centroid : this.centroids) {
            if(centroid != null){
                totalWeight += centroid.weight;
            }
        }

        return totalWeight;
    }

    public static void teste() {
        // Create a new T-Digest with a compression factor of 100
        var tDigest = new TDigest(100);

        // Add data points to the T-Digest
        tDigest.add(10, 1);
        tDigest.add(20, 2);
        tDigest.add(30, 3);
        tDigest.add(40, 4);
        tDigest.add(50, 5);

        // Estimate the 80th percentile
        final int estimatedQuantile = tDigest.estimateQuantile(0.8);
        System.out.println("Estimated 80th percentile:" + estimatedQuantile); // 36.666666666666664
    }
}
